{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "docstore-preprocessed-processed.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-ZlAKhMwuNbD"
      ],
      "authorship_tag": "ABX9TyOULtFBUJnXMogfTdnvDeSf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ctezna/notebooks/blob/main/docstore_preprocessed_processed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJDJfmlaSkLj"
      },
      "source": [
        "# Docstore: Preprocessed to Processed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnn7NlwmSoiP"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9KlLRVfzPJv"
      },
      "source": [
        "import time\n",
        "start = time.time()"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNDpGgfoSPCt",
        "outputId": "0747822b-bb23-47a6-f8b1-58aff9064d29"
      },
      "source": [
        "! pip3 install boto3 s3fs nltk"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (1.17.78)\n",
            "Requirement already satisfied: s3fs in /usr/local/lib/python3.7/dist-packages (2021.5.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.78 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.20.78)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.4.2)\n",
            "Requirement already satisfied: aiobotocore>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from s3fs) (1.3.0)\n",
            "Requirement already satisfied: fsspec==2021.05.0 in /usr/local/lib/python3.7/dist-packages (from s3fs) (2021.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.78->boto3) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.78->boto3) (2.8.1)\n",
            "Requirement already satisfied: wrapt>=1.10.10 in /usr/local/lib/python3.7/dist-packages (from aiobotocore>=1.0.1->s3fs) (1.12.1)\n",
            "Requirement already satisfied: aioitertools>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from aiobotocore>=1.0.1->s3fs) (0.7.1)\n",
            "Requirement already satisfied: aiohttp>=3.3.1 in /usr/local/lib/python3.7/dist-packages (from aiobotocore>=1.0.1->s3fs) (3.7.4.post0)\n",
            "Requirement already satisfied: typing_extensions>=3.7 in /usr/local/lib/python3.7/dist-packages (from aioitertools>=0.5.1->aiobotocore>=1.0.1->s3fs) (3.7.4.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (21.2.0)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (5.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (1.6.3)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw_a7_ZwSsuL"
      },
      "source": [
        "import time\n",
        "start = time.time()"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmxzx5jlS21U"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu9gosz0S4AS"
      },
      "source": [
        "import boto3, botocore\n",
        "\n",
        "def download_files_s3(bucket, files, creds, target_file=None):\n",
        "  s3 = boto3.resource('s3', aws_access_key_id = creds['keyId'], \n",
        "                          aws_secret_access_key = creds['key'],\n",
        "                          aws_session_token = creds['token'])\n",
        "\n",
        "  KEYS = files\n",
        "\n",
        "  for [i, KEY] in enumerate(KEYS):\n",
        "    try:\n",
        "      tgt_file = KEY.split('/')[-1]\n",
        "      if target_file:\n",
        "        tgt_file = target_file.split('.')[0] + str(i) + \\\n",
        "                    '.' + target_file.split('.')[1]\n",
        "      s3.Bucket(BUCKET_NAME).download_file(KEY, tgt_file)\n",
        "      \n",
        "    except botocore.exceptions.ClientError as e:\n",
        "      if e.response['Error']['Code'] == \"404\":\n",
        "        print(\"The object does not exist.\")\n",
        "      else:\n",
        "        raise\n",
        "\n",
        "  return s3\n",
        "\n",
        "def download_files_s3_all(bucket, path, creds, target_file=None):\n",
        "  s3 = boto3.resource('s3', aws_access_key_id = creds['keyId'], \n",
        "                          aws_secret_access_key = creds['key'],\n",
        "                          aws_session_token = creds['token'])\n",
        "  docs = []\n",
        "  for file in s3.Bucket(BUCKET_NAME).objects.filter(Prefix=path).all():\n",
        "      if len(file.key.split('/')[-1]) > 1:\n",
        "        docs.append(file.key)\n",
        "\n",
        "  KEYS = docs\n",
        "\n",
        "  for [i, KEY] in enumerate(KEYS):\n",
        "    try:\n",
        "      tgt_file = KEY.split('/')[-1]\n",
        "      if target_file:\n",
        "        tgt_file = target_file.split('.')[0] + str(i) + \\\n",
        "                    '.' + target_file.split('.')[1]\n",
        "      s3.Bucket(BUCKET_NAME).download_file(KEY, tgt_file)\n",
        "      \n",
        "    except botocore.exceptions.ClientError as e:\n",
        "      if e.response['Error']['Code'] == \"404\":\n",
        "        print(\"The object does not exist.\")\n",
        "      else:\n",
        "        raise\n",
        "  return docs\n",
        "\n",
        "def upload_files_s3(bucket, files, creds, zone):\n",
        "  s3 = boto3.resource('s3', aws_access_key_id = creds['keyId'], \n",
        "                          aws_secret_access_key = creds['key'],\n",
        "                          aws_session_token = creds['token'])\n",
        "\n",
        "  KEYS = files\n",
        "\n",
        "  for KEY in KEYS:\n",
        "    try:\n",
        "      target_file = zone + KEY.split('/')[-1]\n",
        "      s3.Bucket(BUCKET_NAME).upload_file(KEY, target_file)\n",
        "      \n",
        "    except botocore.exceptions.ClientError as e:\n",
        "      if e.response['Error']['Code'] == \"404\":\n",
        "        print(\"The object does not exist.\")\n",
        "      else:\n",
        "        raise"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZLcs27aS6L7"
      },
      "source": [
        "BUCKET_NAME = 'docstore-datalake'\n",
        "\n",
        "awskey = \"\"\"aws_access_key_id=ASIA4ZNZQGFYXCL2QFTS\n",
        "aws_secret_access_key=mgaP+y5uKBSh3acNnntfH91uydBetHJcldUtM0ip\n",
        "aws_session_token=IQoJb3JpZ2luX2VjEBwaCXVzLXdlc3QtMiJIMEYCIQDBX+RfPJOK4s5WtHan7To8W/4fpikiqdxV4VAoRQlWlAIhANfp+cAr1BHz6stlHwPfhp+xCOKgv1LjI0d2BPsl7kBwKrsCCLX//////////wEQABoMODc5MjQ2NzE3Mjk3IgwSVSXIhtAZxgCjPcwqjwKgSgaUV7B6Dl0eBznBKOIbYIc80w7vk2X/JFrnj4dZTlMKqOlZ44YcJSUR8g7lnjOisQ60kSMdP1hjjybIpRMsTyZM8/mw7jvuyfbaRPgRsl22i5qB2o8S5G3IB83mrc3WPlcRa6KiI5pvcri3M9CSH8ILKp8bnEHyp4K34QD6ZsrWA+dKxmJpdwp2C+qHygqN2o4rTXYw64YKDIDIusgwBmUs9+7dQ8rLq0ejy0bsosUFTOoKMR/n7WffZ9O6iYUplB+Y55S9XQrEyG3Zhw2DXYcLQrpLgWuQuTrOZq9MCqKs7l9UkRhQ2c9JzW8QZZUnEMpL7D0WhDTZQTM7CkderUYWdaF5QImCZspDPjd5MKHzoYUGOpwB9V+1U59DC1/5ltUXnbmoH4QpE0GwK0lofQgAPYuKefLcIj8aLtaQEkjOHskq5KOcQswpULAup1kgCY/FdHuYW2C7lnd7kJyfKubL0ydfxH9KN+lEov1c3OfHZOiPP3wHldap0McDmjiT/HVuOKswXpQiShKdNX5e3PoWKt7cSpEUHoURHaGIeMNmF7qk9zfog6kk12Ee0SP7WFaM\"\"\"\n",
        "\n",
        "creds = {\n",
        "        'keyId': awskey.split('\\n')[0].split('=')[-1],\n",
        "        'key' : awskey.split('\\n')[1].split('=')[-1],\n",
        "        'token' : awskey.split('\\n')[2].split('=')[-1]\n",
        "        }\n",
        "\n",
        "financialNews = download_files_s3_all(BUCKET_NAME, '02-preprocessed/financialNews/', creds, target_file='financialNews.parquet')\n",
        "movieReviews = download_files_s3_all(BUCKET_NAME, '02-preprocessed/movieReviews/', creds, target_file='movieReviews.parquet')\n",
        "twitterSent = download_files_s3_all(BUCKET_NAME, '02-preprocessed/twitterSentiment/', creds, target_file='twitterSentiment.parquet')\n",
        "covidVaccine = download_files_s3_all(BUCKET_NAME, '02-preprocessed/covidVaccine/', creds, target_file='covidVaccine.parquet')"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jLlk-IbUUaZ"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJfvCcNgUXo7"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "financialDf = pd.read_parquet('financialNews0.parquet')\n",
        "movieReviewDf = pd.read_parquet('movieReviews0.parquet')\n",
        "twitterSentDf = pd.read_parquet('twitterSentiment0.parquet')\n",
        "covidVaxDf = pd.read_parquet('covidVaccine0.parquet')\n",
        "\n",
        "financialDf.columns = ['text', 'sentiment']\n",
        "movieReviewDf.columns = ['text', 'sentiment']"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D49Uk5rnfO82"
      },
      "source": [
        "## Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUubWZaifRbx",
        "outputId": "2020c075-b484-4e5e-889c-403bed81e434"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "eng_stopwords = stopwords.words('english')\n",
        "\n",
        "stop_words = set(eng_stopwords)\n",
        "punctuation = punctuation + '\\n' + '—' + '“' + ',' + '”' + '‘' + '-' + '’'\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYpvOYJikbZO"
      },
      "source": [
        "contractions_dict = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"doesn’t\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"don’t\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y’all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\",\n",
        "\"ain’t\": \"am not\",\n",
        "\"aren’t\": \"are not\",\n",
        "\"can’t\": \"cannot\",\n",
        "\"can’t’ve\": \"cannot have\",\n",
        "\"’cause\": \"because\",\n",
        "\"could’ve\": \"could have\",\n",
        "\"couldn’t\": \"could not\",\n",
        "\"couldn’t’ve\": \"could not have\",\n",
        "\"didn’t\": \"did not\",\n",
        "\"doesn’t\": \"does not\",\n",
        "\"don’t\": \"do not\",\n",
        "\"don’t\": \"do not\",\n",
        "\"hadn’t\": \"had not\",\n",
        "\"hadn’t’ve\": \"had not have\",\n",
        "\"hasn’t\": \"has not\",\n",
        "\"haven’t\": \"have not\",\n",
        "\"he’d\": \"he had\",\n",
        "\"he’d’ve\": \"he would have\",\n",
        "\"he’ll\": \"he will\",\n",
        "\"he’ll’ve\": \"he will have\",\n",
        "\"he’s\": \"he is\",\n",
        "\"how’d\": \"how did\",\n",
        "\"how’d’y\": \"how do you\",\n",
        "\"how’ll\": \"how will\",\n",
        "\"how’s\": \"how is\",\n",
        "\"i’d\": \"i would\",\n",
        "\"i’d’ve\": \"i would have\",\n",
        "\"i’ll\": \"i will\",\n",
        "\"i’ll’ve\": \"i will have\",\n",
        "\"i’m\": \"i am\",\n",
        "\"i’ve\": \"i have\",\n",
        "\"isn’t\": \"is not\",\n",
        "\"it’d\": \"it would\",\n",
        "\"it’d’ve\": \"it would have\",\n",
        "\"it’ll\": \"it will\",\n",
        "\"it’ll’ve\": \"it will have\",\n",
        "\"it’s\": \"it is\",\n",
        "\"let’s\": \"let us\",\n",
        "\"ma’am\": \"madam\",\n",
        "\"mayn’t\": \"may not\",\n",
        "\"might’ve\": \"might have\",\n",
        "\"mightn’t\": \"might not\",\n",
        "\"mightn’t’ve\": \"might not have\",\n",
        "\"must’ve\": \"must have\",\n",
        "\"mustn’t\": \"must not\",\n",
        "\"mustn’t’ve\": \"must not have\",\n",
        "\"needn’t\": \"need not\",\n",
        "\"needn’t’ve\": \"need not have\",\n",
        "\"o’clock\": \"of the clock\",\n",
        "\"oughtn’t\": \"ought not\",\n",
        "\"oughtn’t’ve\": \"ought not have\",\n",
        "\"shan’t\": \"shall not\",\n",
        "\"sha’n’t\": \"shall not\",\n",
        "\"shan’t’ve\": \"shall not have\",\n",
        "\"she’d\": \"she would\",\n",
        "\"she’d’ve\": \"she would have\",\n",
        "\"she’ll\": \"she will\",\n",
        "\"she’ll’ve\": \"she will have\",\n",
        "\"she’s\": \"she is\",\n",
        "\"should’ve\": \"should have\",\n",
        "\"shouldn’t\": \"should not\",\n",
        "\"shouldn’t’ve\": \"should not have\",\n",
        "\"so’ve\": \"so have\",\n",
        "\"so’s\": \"so is\",\n",
        "\"that’d\": \"that would\",\n",
        "\"that’d’ve\": \"that would have\",\n",
        "\"that’s\": \"that is\",\n",
        "\"there’d\": \"there would\",\n",
        "\"there’d’ve\": \"there would have\",\n",
        "\"there’s\": \"there is\",\n",
        "\"they’d\": \"they would\",\n",
        "\"they’d’ve\": \"they would have\",\n",
        "\"they’ll\": \"they will\",\n",
        "\"they’ll’ve\": \"they will have\",\n",
        "\"they’re\": \"they are\",\n",
        "\"they’ve\": \"they have\",\n",
        "\"to’ve\": \"to have\",\n",
        "\"wasn’t\": \"was not\",\n",
        "\"we’d\": \"we would\",\n",
        "\"we’d’ve\": \"we would have\",\n",
        "\"we’ll\": \"we will\",\n",
        "\"we’ll’ve\": \"we will have\",\n",
        "\"we’re\": \"we are\",\n",
        "\"we’ve\": \"we have\",\n",
        "\"weren’t\": \"were not\",\n",
        "\"what’ll\": \"what will\",\n",
        "\"what’ll’ve\": \"what will have\",\n",
        "\"what’re\": \"what are\",\n",
        "\"what’s\": \"what is\",\n",
        "\"what’ve\": \"what have\",\n",
        "\"when’s\": \"when is\",\n",
        "\"when’ve\": \"when have\",\n",
        "\"where’d\": \"where did\",\n",
        "\"where’s\": \"where is\",\n",
        "\"where’ve\": \"where have\",\n",
        "\"who’ll\": \"who will\",\n",
        "\"who’ll’ve\": \"who will have\",\n",
        "\"who’s\": \"who is\",\n",
        "\"who’ve\": \"who have\",\n",
        "\"why’s\": \"why is\",\n",
        "\"why’ve\": \"why have\",\n",
        "\"will’ve\": \"will have\",\n",
        "\"won’t\": \"will not\",\n",
        "\"won’t’ve\": \"will not have\",\n",
        "\"would’ve\": \"would have\",\n",
        "\"wouldn’t\": \"would not\",\n",
        "\"wouldn’t’ve\": \"would not have\",\n",
        "\"y’all\": \"you all\",\n",
        "\"y’all\": \"you all\",\n",
        "\"y’all’d\": \"you all would\",\n",
        "\"y’all’d’ve\": \"you all would have\",\n",
        "\"y’all’re\": \"you all are\",\n",
        "\"y’all’ve\": \"you all have\",\n",
        "\"you’d\": \"you would\",\n",
        "\"you’d’ve\": \"you would have\",\n",
        "\"you’ll\": \"you will\",\n",
        "\"you’ll’ve\": \"you will have\",\n",
        "\"you’re\": \"you are\",\n",
        "\"you’re\": \"you are\",\n",
        "\"you’ve\": \"you have\",\n",
        "}\n",
        "\n",
        "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcd8U1eVxp68"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZlAKhMwuNbD"
      },
      "source": [
        "### Previews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LwMB1yY9WCpx",
        "outputId": "08b70998-18b5-4f58-a0da-806b81ee616f"
      },
      "source": [
        "financialDf.head()"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MNTA Over 12.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>OI  Over 21.37</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1\n",
              "1  user: AAP MOVIE. 55% return for the FEA/GEED i...          1\n",
              "2  user I'd be afraid to short AMZN - they are lo...          1\n",
              "3                                  MNTA Over 12.00            1\n",
              "4                                   OI  Over 21.37            1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uBo-VpGggZp",
        "outputId": "1349ce4e-1d93-4705-85b4-4ec393f5b062"
      },
      "source": [
        "print(financialDf.shape)\n",
        "financialDf.sentiment.value_counts()"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5791, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1    3685\n",
              "-1    2106\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8rPP5M_QgOW3",
        "outputId": "06838f55-c146-4306-dc87-3f166fe269e2"
      },
      "source": [
        "movieReviewDf.head()"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IxNQ2VbgjyS",
        "outputId": "dd1707e2-bf66-4e1c-d817-33610704663c"
      },
      "source": [
        "print(movieReviewDf.shape)\n",
        "movieReviewDf.sentiment.value_counts()"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    25000\n",
              "negative    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sOjQkgjBgQIW",
        "outputId": "bab230d2-689e-46d2-8b9f-2afbc41824c4"
      },
      "source": [
        "twitterSentDf.tail()"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601966</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>AmandaMarie1028</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601969</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>TheWDBoards</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601991</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>bpbabe</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>4</td>\n",
              "      <td>2193602064</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>tinydiamondz</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>4</td>\n",
              "      <td>2193602129</td>\n",
              "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>RyanTrevMorris</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         sentiment  ...                                              tweet\n",
              "1599995          4  ...  Just woke up. Having no school is the best fee...\n",
              "1599996          4  ...  TheWDB.com - Very cool to hear old Walt interv...\n",
              "1599997          4  ...  Are you ready for your MoJo Makeover? Ask me f...\n",
              "1599998          4  ...  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "1599999          4  ...  happy #charitytuesday @theNSPCC @SparksCharity...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv9u2GPugSSA",
        "outputId": "6a1a866b-0e4a-4a6d-e789-5d03c75aa408"
      },
      "source": [
        "print(twitterSentDf.shape)\n",
        "twitterSentDf.sentiment.value_counts()"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1600000, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    800000\n",
              "0    800000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iMsXrdx5ie1g",
        "outputId": "178706dc-bd54-484d-abb0-09aa79c2f7ab"
      },
      "source": [
        "covidVaxDf.head()"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_name</th>\n",
              "      <th>user_location</th>\n",
              "      <th>user_description</th>\n",
              "      <th>user_created</th>\n",
              "      <th>user_followers</th>\n",
              "      <th>user_friends</th>\n",
              "      <th>user_favourites</th>\n",
              "      <th>user_verified</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>source</th>\n",
              "      <th>is_retweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MyNewsNE</td>\n",
              "      <td>Assam</td>\n",
              "      <td>MyNewsNE a dedicated multi-lingual media house...</td>\n",
              "      <td>24-05-2020 10:18</td>\n",
              "      <td>64.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>False</td>\n",
              "      <td>18-08-2020 12:55</td>\n",
              "      <td>Australia to Manufacture Covid-19 Vaccine and ...</td>\n",
              "      <td>['CovidVaccine']</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Shubham Gupta</td>\n",
              "      <td>nan</td>\n",
              "      <td>I will tell about all experiences of my life f...</td>\n",
              "      <td>14-08-2020 16:42</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>18-08-2020 12:55</td>\n",
              "      <td>#CoronavirusVaccine #CoronaVaccine #CovidVacci...</td>\n",
              "      <td>['CoronavirusVaccine', 'CoronaVaccine', 'Covid...</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Journal of Infectiology</td>\n",
              "      <td>nan</td>\n",
              "      <td>Journal of Infectiology (ISSN 2689-9981) is ac...</td>\n",
              "      <td>14-12-2017 07:07</td>\n",
              "      <td>143.0</td>\n",
              "      <td>566.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>False</td>\n",
              "      <td>18-08-2020 12:46</td>\n",
              "      <td>Deaths due to COVID-19 in Affected Countries\\n...</td>\n",
              "      <td>nan</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Zane</td>\n",
              "      <td>nan</td>\n",
              "      <td>Fresher than you.</td>\n",
              "      <td>18-09-2019 11:01</td>\n",
              "      <td>29.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>620.0</td>\n",
              "      <td>False</td>\n",
              "      <td>18-08-2020 12:45</td>\n",
              "      <td>@Team_Subhashree @subhashreesotwe @iamrajchoco...</td>\n",
              "      <td>nan</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ann-Maree O’Connor</td>\n",
              "      <td>Adelaide, South Australia</td>\n",
              "      <td>Retired university administrator. Melburnian b...</td>\n",
              "      <td>24-01-2013 14:53</td>\n",
              "      <td>83.0</td>\n",
              "      <td>497.0</td>\n",
              "      <td>10737.0</td>\n",
              "      <td>False</td>\n",
              "      <td>18-08-2020 12:45</td>\n",
              "      <td>@michellegrattan @ConversationEDU This is what...</td>\n",
              "      <td>nan</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 user_name  ... is_retweet\n",
              "0                 MyNewsNE  ...      False\n",
              "1            Shubham Gupta  ...      False\n",
              "2  Journal of Infectiology  ...      False\n",
              "3                     Zane  ...      False\n",
              "4       Ann-Maree O’Connor  ...      False\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pIIDZo1iiPs",
        "outputId": "72994f19-7b50-4b2a-d4e4-d90b8758db03"
      },
      "source": [
        "print(covidVaxDf.shape)"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(207006, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw3NjlbtiViD"
      },
      "source": [
        "### Encode target values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi-PajyXhljI"
      },
      "source": [
        "Changing target sentiment values to 0 (negative) and 1 (positive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFI99urcgnud",
        "outputId": "8f62b303-a0d2-4626-b803-620b32221df7"
      },
      "source": [
        "twitterSentDf.loc[twitterSentDf.sentiment == 4, \"sentiment\"] = 1\n",
        "twitterSentDf.sentiment.value_counts()"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    800000\n",
              "0    800000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86qfPiEplj0n",
        "outputId": "130dcda8-0b69-42b5-edba-a611e770a5fb"
      },
      "source": [
        "vals = {'positive':1, 'negative':0}\n",
        "movieReviewDf.sentiment = movieReviewDf.sentiment.apply(lambda x: vals[x])\n",
        "movieReviewDf.sentiment.value_counts()"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    25000\n",
              "0    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9CAY9qzl0Ed",
        "outputId": "612ca3fa-8fd0-4910-c38b-d86a2e391ecb"
      },
      "source": [
        "financialDf.loc[financialDf.sentiment == -1, \"sentiment\"] = 0\n",
        "financialDf.sentiment.value_counts()"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    3685\n",
              "0    2106\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNTLAhJ1iY4q"
      },
      "source": [
        "Remove unneeded columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HuikY5-iASu"
      },
      "source": [
        "try:\n",
        "  twitterSentDf.drop(columns=['id', 'query'], inplace=True)\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhW_a3PDjIWO"
      },
      "source": [
        "Preparing a datasets to be used in model development and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUNdmKsdm9PG"
      },
      "source": [
        "twitterDf = twitterSentDf[['tweet', 'sentiment']]\n",
        "twitterDf.columns = ['text', 'sentiment']"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDPGWBIkjB3z"
      },
      "source": [
        "#dfs = [financialDf, movieReviewDf, twitterDf.sample(frac=1)[:50000]]\n",
        "dfs = [twitterDf.sample(frac=1)[:50000]]"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgP1nD0aoUYT",
        "outputId": "cda9cb2d-5312-45a8-97f7-827b8508cef1"
      },
      "source": [
        "model_data = pd.concat(dfs)\n",
        "model_data.shape"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cATrbS43QV29"
      },
      "source": [
        "### Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBeTUT0Cp2Yd"
      },
      "source": [
        "def clean_text(text, stem=False, lem=False):\n",
        "  stemmer = SnowballStemmer('english')\n",
        "  text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "  lemma = nltk.wordnet.WordNetLemmatizer()\n",
        "  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
        "  tokens = []\n",
        "  for token in text.split():\n",
        "    if stem:\n",
        "      tokens.append(stemmer.stem(token))\n",
        "    elif lem:\n",
        "      tokens.append(lemma.lemmatize(token))\n",
        "    else:\n",
        "      tokens.append(token)\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "# Function to clean the html from the article\n",
        "def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return cleantext\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "# Function expand the contractions if there's any\n",
        "def expand_contractions(s, contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, s)\n",
        "\n",
        "# Function to preprocess the texts\n",
        "def preprocessing(text, stopwords=True, stem=False, lem=False):\n",
        "    global text_sent\n",
        "    \n",
        "    # Converting to lowercase\n",
        "    text = text.str.lower()\n",
        "    \n",
        "    # Removing the HTML\n",
        "    text = text.apply(lambda x: cleanhtml(x))\n",
        "\n",
        "    # Removing square brackets\n",
        "    text = text.apply(lambda x: remove_between_square_brackets(x))\n",
        "    \n",
        "    # Removing the email ids\n",
        "    text = text.apply(lambda x: re.sub('\\S+@\\S+','', x))\n",
        "    \n",
        "    # Removing The URLS\n",
        "    text = text.apply(lambda x: re.sub(\"((http\\://|https\\://|ftp\\://)|(www.))+(([a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,4})|([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}))(/[a-zA-Z0-9%:/-_\\?\\.'~]*)?\",'', x))\n",
        "    \n",
        "    # Removing the '\\xa0'\n",
        "    text = text.apply(lambda x: x.replace(\"\\xa0\", \" \"))\n",
        "    \n",
        "    # Removing the contractions\n",
        "    text = text.apply(lambda x: expand_contractions(x))\n",
        "    \n",
        "    # Stripping the possessives\n",
        "    text = text.apply(lambda x: x.replace(\"'s\", ''))\n",
        "    text = text.apply(lambda x: x.replace('’s', ''))\n",
        "    text = text.apply(lambda x: x.replace(\"\\'s\", ''))\n",
        "    text = text.apply(lambda x: x.replace(\"\\’s\", ''))\n",
        "    \n",
        "    # Removing the Trailing and leading whitespace and double spaces\n",
        "    text = text.apply(lambda x: re.sub(' +', ' ',x))\n",
        "    \n",
        "    # Copying the text for the sentence tokenization\n",
        "    text_sent = text.copy()\n",
        "    \n",
        "    # Removing punctuations from the text\n",
        "    text = text.apply(lambda x: ''.join(word for word in x if word not in punctuation))\n",
        "    \n",
        "    # Removing the Trailing and leading whitespace and double spaces again as removing punctuation might\n",
        "    # Lead to a white space\n",
        "    text = text.apply(lambda x: re.sub(' +', ' ',x))\n",
        "    \n",
        "    # Removing the Stopwords\n",
        "    if stopwords:\n",
        "      text = text.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
        "\n",
        "    text = text.apply(lambda x: clean_text(x, stem, lem))\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmDY3S9tW2EP",
        "outputId": "bfcb1ea4-93d8-4fbc-840b-dc9cc1adaa14"
      },
      "source": [
        "%%time\n",
        "model_data['processed_text'] = preprocessing(model_data.text, False, False, True)\n",
        "model_data['processed_text_stopwords'] = preprocessing(model_data.text, True, False, True)"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 13 s, sys: 60 ms, total: 13.1 s\n",
            "Wall time: 13.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "jX5ckQxci8ci",
        "outputId": "e7c269b6-a1c1-44aa-dd59-1491288009a5"
      },
      "source": [
        "model_data"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>processed_text</th>\n",
              "      <th>processed_text_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>980192</th>\n",
              "      <td>@JBeauty life is about all the catchin up that...</td>\n",
              "      <td>1</td>\n",
              "      <td>jbeauty life is about all the catchin up that ...</td>\n",
              "      <td>jbeauty life catchin 2 donewide awake n buzy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135666</th>\n",
              "      <td>can't believe she had such a frustrating work ...</td>\n",
              "      <td>0</td>\n",
              "      <td>cannot believe she had such a frustrating work...</td>\n",
              "      <td>cannot believe frustrating work conversation c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>902296</th>\n",
              "      <td>@lalaland_city Hey, u like FOB 2! Follow me, l...</td>\n",
              "      <td>1</td>\n",
              "      <td>lalalandcity hey u like fob 2 follow me luv 2 ...</td>\n",
              "      <td>lalalandcity hey u like fob 2 follow luv 2 tal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>841019</th>\n",
              "      <td>is amazed at how God gives us rain and sunshin...</td>\n",
              "      <td>1</td>\n",
              "      <td>is amazed at how god give u rain and sunshine ...</td>\n",
              "      <td>amazed god give u rain sunshine one day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503966</th>\n",
              "      <td>Damn Says I've Haven't Been On Twitter In A Wh...</td>\n",
              "      <td>0</td>\n",
              "      <td>damn say i have have not been on twitter in a ...</td>\n",
              "      <td>damn say twitter whole week whats really goin ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148729</th>\n",
              "      <td>@Peezle haha my gf fell asleep on the couch.. ...</td>\n",
              "      <td>0</td>\n",
              "      <td>peezle haha my gf fell asleep on the couch do ...</td>\n",
              "      <td>peezle haha gf fell asleep couch sometimes fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320936</th>\n",
              "      <td>$256 and $33. I will not be so careless next t...</td>\n",
              "      <td>0</td>\n",
              "      <td>256 and 33 i will not be so careless next time</td>\n",
              "      <td>256 33 careless next time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124349</th>\n",
              "      <td>Cant believe im working today</td>\n",
              "      <td>0</td>\n",
              "      <td>cant believe im working today</td>\n",
              "      <td>cant believe im working today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498730</th>\n",
              "      <td>@Hernamewaslolo Disneyland is full. No kidding...</td>\n",
              "      <td>0</td>\n",
              "      <td>hernamewaslolo disneyland is full no kidding w...</td>\n",
              "      <td>hernamewaslolo disneyland full kidding turning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>729226</th>\n",
              "      <td>I'm sick.  I wanna say fuck but it's not ladyl...</td>\n",
              "      <td>0</td>\n",
              "      <td>i am sick i wanna say fuck but it is not ladyl...</td>\n",
              "      <td>sick wanna say fuck ladylike oops</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  ...                           processed_text_stopwords\n",
              "980192  @JBeauty life is about all the catchin up that...  ...       jbeauty life catchin 2 donewide awake n buzy\n",
              "135666  can't believe she had such a frustrating work ...  ...  cannot believe frustrating work conversation c...\n",
              "902296  @lalaland_city Hey, u like FOB 2! Follow me, l...  ...  lalalandcity hey u like fob 2 follow luv 2 tal...\n",
              "841019  is amazed at how God gives us rain and sunshin...  ...            amazed god give u rain sunshine one day\n",
              "503966  Damn Says I've Haven't Been On Twitter In A Wh...  ...  damn say twitter whole week whats really goin ...\n",
              "...                                                   ...  ...                                                ...\n",
              "148729  @Peezle haha my gf fell asleep on the couch.. ...  ...  peezle haha gf fell asleep couch sometimes fee...\n",
              "320936  $256 and $33. I will not be so careless next t...  ...                          256 33 careless next time\n",
              "124349                     Cant believe im working today   ...                      cant believe im working today\n",
              "498730  @Hernamewaslolo Disneyland is full. No kidding...  ...  hernamewaslolo disneyland full kidding turning...\n",
              "729226  I'm sick.  I wanna say fuck but it's not ladyl...  ...                  sick wanna say fuck ladylike oops\n",
              "\n",
              "[50000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hia5SWY9TXSt"
      },
      "source": [
        "data = covidVaxDf.sample(100).text.values.tolist()"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I15Bhexvw9e6"
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xrsLnWzL63j"
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2BNnF2YzP3V"
      },
      "source": [
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiihOrGmzZ_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257ec945-6068-4467-e33d-8aed403adbfc"
      },
      "source": [
        "%%time\n",
        "\n",
        "import spacy\n",
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams)"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 15.3 s, sys: 153 ms, total: 15.5 s\n",
            "Wall time: 15.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rFk4mJKNkpG",
        "outputId": "a17737f1-baba-4378-e9b1-8b81de2baae0"
      },
      "source": [
        "%%time\n",
        "\n",
        "import gensim.corpora as corpora\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11.2 ms, sys: 13 ms, total: 24.2 ms\n",
            "Wall time: 25.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F48xkaHmYWTi"
      },
      "source": [
        "## Save Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaS3KhLHYYYV"
      },
      "source": [
        "model_data.to_parquet('model_data.parquet')\n",
        "covidVaxDf.to_parquet('covidVaccine.parquet')"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US17O8qKVGwi"
      },
      "source": [
        "with open('clean_data.txt', 'w') as filehandle:\n",
        "    filehandle.writelines(\"%s\\n\" % c for c in data_lemmatized)"
      ],
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbhu8fplXrOK"
      },
      "source": [
        "## Upload Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WYYQnAjPOYXX",
        "outputId": "6345fffc-8fdc-4721-b551-fc4f60f8181c"
      },
      "source": [
        "import datetime\n",
        "\n",
        "year = datetime.datetime.now().year\n",
        "month = datetime.datetime.now().month\n",
        "day = datetime.datetime.now().day\n",
        "f'05-archive/{year}/{month}/{day}/'"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'05-archive/2021/5/22/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKrZogiOYDpE"
      },
      "source": [
        "files = ['./model_data.parquet', './covidVaccine.parquet', './clean_data.txt']\n",
        "\n",
        "for f in files:\n",
        "  upload_files_s3(BUCKET_NAME, [f], creds, '03-processed{}/'.format(f.split('.')[-2]))\n",
        "  upload_files_s3(BUCKET_NAME, [f], creds, f'05-archive/{year}/{month}/{day}/')"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlMOSWSrzFsc",
        "outputId": "a2001dc4-9f8e-4bf3-9441-4dfd9a7bf51d"
      },
      "source": [
        "end = time.time()\n",
        "print('Run time: ', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run time:  41.8952739238739\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}